{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Tokenization is the process of breaking down text data into smaller, manageable units called tokens, which can be words, phrases, subwords, or even individual characters. This is typically the first step in preprocessing text for machine learning (ML) and natural language processing (NLP) tasks, as it transforms raw text into a format that algorithms can analyze and understand.\n",
    "\n",
    "\n",
    "Tokenization - https://tiktokenizer.vercel.app/\n",
    "\n",
    "## Why is Tokenization Important ? \n",
    "\n",
    "\n",
    "- Text to Numbers: In machine learning models operate on numerical data, not raw text. Tokenization converts text into tokens, which are then mapped to numbers so that models can process them.\n",
    "\n",
    "- Pattern Recognition: By splitting text into tokens, algorithms can more easily identify patterns, relationships, and context within the data.\n",
    "\n",
    "- Efficiency: Tokenization makes it possible to handle large volumes of text efficiently, optimizing memory usage and computational speed-especially important in large language models..\n",
    "\n",
    "\n",
    "- Generalization: Good tokenization strategies, such as subword or character tokenization, allow models to handle new or rare words by breaking them into familiar components.\n",
    "\n",
    "\n",
    "## Types of Tokenization:\n",
    "\n",
    "\n",
    "-  Word Tokenization: Splits text into individual words.\n",
    "Example: \"Chatbots are helpful.\" → [\"Chatbots\", \"are\", \"helpful\"]\n",
    "\n",
    "- Character Tokenization: Breaks text into individual characters.\n",
    "Example: \"Chatbots\" → [\"C\", \"h\", \"a\", \"t\", \"b\", \"o\", \"t\", \"s\"]\n",
    "\n",
    "- Subword Tokenization: Splits words into smaller units (subwords), useful for handling rare or unknown words.\n",
    "Example: \"unhappiness\" → [\"un\", \"happiness\"] or [\"un\", \"hap\", \"pi\", \"ness\"]\n",
    "\n",
    "- Sentence Tokenization: Divides text into sentences, often used for tasks like summarization or translation.\n",
    "\n",
    "\n",
    "## Tokenization for LLM's:\n",
    "\n",
    "\n",
    "- Tokenization is the gateway for all LLM operations-text is tokenized, converted to embeddings, processed, and then detokenized for output.\n",
    "\n",
    "\n",
    "- LLMs often use subword tokenization (like Byte-Pair Encoding or WordPiece) to balance vocabulary size, efficiency, and the ability to handle new words.\n",
    "\n",
    "\n",
    "- Tokenization quality directly impacts the model’s ability to understand context, manage multilingual data, and generate accurate responses.\n",
    "\n",
    "  \n",
    "\n",
    "## Popular Tokenization Algorithms\n",
    "\n",
    "\n",
    "| **Algorithm/**              | **Description & Approach**                                                                            | **Typical Use Cases & Models**                                                                  |\n",
    "|----------------------------------|------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------|\n",
    "| **Whitespace/Regex**             | Splits text based on spaces or regular expressions.                                                   | Simple NLP tasks, preprocessing, rule-based systems.                                           |\n",
    "| **Word Tokenizers**              | Divides text into words, often using libraries like NLTK, SpaCy, or Keras.                           | Text classification, sentiment analysis, topic modeling (Gensim), general NLP pipelines.       |\n",
    "| **Character Tokenizers**        | Splits text into individual characters.                                                               | Handling misspellings, rare words, languages without clear word boundaries, deep learning models.|\n",
    "| **Byte-Pair Encoding (BPE)**     | Iteratively merges frequent pairs of bytes/characters to form subwords.                              | Neural machine translation, GPT-2, RoBERTa, multilingual models.                             |\n",
    "| **WordPiece**                    | Similar to BPE, but merges pairs that maximize likelihood of training data.                           | BERT, DistilBERT, Electra, other transformer models.                                            |\n",
    "| **Unigram**                      | Starts with a large vocabulary, then prunes to optimize likelihood.                                  | Used with SentencePiece in models like T5, XLNet, ALBERT.                                      |\n",
    "| **SentencePiece**                | Unsupervised, language-independent tokenizer supporting BPE and Unigram.                             | Neural machine translation, text generation, T5, XLNet; supports multiple languages.          |\n",
    "| **Hugging Face Transformers**    | Library providing fast, model-specific tokenizers (BPE, WordPiece, Unigram).                         | BERT, GPT, RoBERTa, T5, and other transformer-based models.                                     |\n",
    "| **Gensim Tokenizer**             | Tokenization for large-scale topic modeling and document similarity.                                 | Topic modeling, information retrieval.                                                        |\n",
    "| **Keras Tokenizer**              | Converts text to sequences for deep learning pipelines.                                               | Text classification, sequence modeling in Keras and TensorFlow.                               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
